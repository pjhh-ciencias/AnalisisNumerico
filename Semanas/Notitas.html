<!DOCTYPE html PUBLIC >

<html lang="es">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <!-- Following part is mathjax, for latex-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <!-- This part is for jQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>


    <!-- This part is to load main.js -->
    <script type="text/javascript" src="../main.js"></script>




    <!-- Next part is for new coomands -->
    <script>
      window.MathJax = {
        tex: {
          macros: {
            sen: "\\operatorname{sen}",
            seg: ["\\overrightarrow{#1}", 1]
          },
          tags: "ams" /* this part is for numbered equations */
        }
      };
    </script>

    <!-- This part is for using single $ for latex input, instead of \(\) -->
    <script>
        MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
        svg: {
            fontCache: 'global'
        }
    };
    </script>



    <!-- CSS -->

    <link rel="stylesheet" , href="../style.css" />

    <!-- This part restart counter of cards to start at n+1 -->
    <style>
        body.number-title{
          counter-reset: sectionCounter 0 cardCounter ;
        }
        h1.number-title{
          counter-reset: sectionCounter 0 cardCounter ;
        }
      </style>
  
    <!-- Top Menu  -->


    <header class="main-header">
        <h1> Análisis Númerico  </h1>
       <nav class="top-nav">
        <ul>
          <li> <a href="https://pjhh-ciencias.github.io/AnalisisNumerico/Home.html"> Inicio  </a> </li>
        <li> <a href="https://pjhh-ciencias.github.io/AnalisisNumerico/Informacion_General.html"> Información general </a> </li>
        <li> <a href="https://pjhh-ciencias.github.io/AnalisisNumerico/Notas.html"> Notas </a> </li>
        <li> <a href="https://pjhh-ciencias.github.io/AnalisisNumerico/Ejercicios.html"> Ejercicios </a> </li>
        <li> <a href="https://pjhh-ciencias.github.io/AnalisisNumerico/Enlaces_externos.html"> Links </a> </li>
        </ul>

       </nav>
       <!-- 
        <nav class="small-nav" align=right>
          <button onclick="myFunction()">Claro/Obscuro 
        </nav> 
      -->

      </header>

      <!-- This part is for title in tab-->
      <title> Notitas   </title>
    
  </head>



  <body class="fondo-body" >
    <h1 class="number-title flexbox"> Notitas </h1>   


    <h1 class="number-title flexbox"> Orden de aproximación $O(h^{n})$ </h1>   


    <div class="nota-box"> <h2 class="number-title"> Introducción</h2>
    <p>
  Esta claro que las sucesiones $\left \{\displaystyle{\frac{1}{n^{2}}}\right\}_{n=1}^{\infty}$ y
  $\left \{\displaystyle{\frac{1}{n}} \right\}_{n=1}^{\infty}$  son ambas sucesiones convergentes  y 
  convergen a cero, sin embargo, notemos que la primera sucesión crece converge a cero más 
  rapidamente que la segunda.
    </p>
    </div>



    <div class="nota-box"> <h2 class="number-title"> Definición</h2>
      <p>Se dice que una función $f(h)$ es de orden $g(h)$ cuando $h \to 0$, lo que se denota 
        por $f(h)=O(g(h))$ (se le llama notación O mayuscula de Landau), si existen 
        constantes $C$ y $c$ tales que:
        $$|f(h)| \leq C|g(h)| \text{ siempre que } |h|< c $$
      </p>
    </div>


    <div class="nota-box"> <h2 class="number-title"> Ejemplo</h2>
      <p>
        Consideremos las funciones $f(x)= x^{3} + 2x^{2} $ y $g(x)=x^{2}$. Entonces $f(x)=O(g(x))$
      </p>
      <p><b>Solución:</b></p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Teorema</h2>
      <p>
        Se satsifacen las siguientes proposiciones:
        <p>
          <ol>
            <li>Si $f_{1}(h) = O(g(h))$ y $f_{2}(h)=O(g(h))$, entonces $f_{1}(h) + f_{2}(h) = O(g(h))$</li>   <br>         
            <li> Si $f_{1}(h) = O(g_{1}(h))$ y $f_{2}(h)=O(g_{2}(h))$, entonces $f_{1}(h) + f_{2}(h) = O(\max\{g_{1}(h), g_{2}(h)\})$</li>   <br>     
            <li>Si $f(h) = O(g(h))$ y $k>0$ es una constante positiva, entonces  $kf(h) = O(g(h))$</li><br>
            <li> Si $f_{1}(h) = O(g_{1}(h))$ y $f_{2}(h)=O(g_{2}(h))$, entonces $f_{1}(h) \cdot f_{2}(h) = O(g_{1}(h) \cdot g_{2}(h))$</li>   <br>     
          </ol></p>
      </p>
      <p><b>Demostración:</b></p>
    </div>


    <div class="nota-box"> <h2 class="number-title"> Observación</h2>
      <p>El teorema anterior se puede resumir de la siguiente manera:

        <ul><li>$O(g(h)) + O(g(h))= O(g(h)) $</li><br>
        <li>$O(g_{1}(h)) + O(g_{2}(h))= O(\max\{g_{1},g_{2}\})$</li><br>
        <li>$k\cdot O(g(h)) = O(g(h))$, para todo constante positiva $k$</li><br>
        <li>$O(g_{1}(h)) \cdot O(g_{2}(h))= O(g_{1}\cdot g_{2})$</li><br>
      </ul>
      </p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Observación</h2>
      <p>
        La notación $O(\cdot)$ (que también se usa para límites en el infinito) proporciona 
        una forma muy útil  para describir  la velocidad de crecimiento (o decrecimiento) de funciones 
        en términos de la velocidad de crecimiento (decrecimiento) de funciones elementales bien conocidas 
        ($x^{n},x^{\frac{1}{n}}, a^{x}, \text{log}_{a},$ etc.)
      </p>

      <p>La velocidad de convergencia de sucesiones puede definirse de forma similar:</p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Observación</h2>
    <p>Sean $\{x_{n}\}_{n=1}^{\infty}$ y $\{y_{n}\}_{n=1}^{\infty}$ dos sucesiones. Se dice que la sucesión 
      $\{x_{n}\}_{n=1}^{\infty}$ es de orden $\{y_{n}\}_{n=1}^{\infty}$, lo que denotamos por $x_{n} =O(y_{n})$, 
      si existen constantes $C$ y $N$ tales que 

      $$|x_{n}| \leq C |y_{n}| \text{ siempre que } n \geq N$$
    
    </p>

    </div>

    <div class="nota-box"> <h2 class="number-title"> Ejemplo</h2>
      <p>
        Consideremos las sucesiones $\left \{\displaystyle{\frac{n^{2}-1}{n^{3}}}\right\}_{n=1}^{\infty}$ y 
        $\left \{\displaystyle{\frac{1}{n}}\right\}_{n=1}^{\infty}$. Entonces 
        $\displaystyle{\frac{n^{2}-1}{n^{3}}}=O\left(\displaystyle{\frac{1}{n}}\right)$
      </p>
      <p><b>Solución:</b></p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Nota </h2>
      <p>
        Con frecuencia nos encontramos con una función $f(h)$ se aproxima mediante 
        otra función $p(h)$ y sabemos que una cota del error cometido es $M|h^{n}|$. Esto
        nos condce a la siguiente definición.
      </p>
    </div>


    <div class="nota-box" id="Def:aprox_O_funciones"> <h2 class="number-title"> Definición</h2>
      <p>
        Supongamos que una función $p(x)$ aproxima a otra función $f(h)$ y que existen una constante 
        real $M>0$ y un número natural  $n$ tales que: 
        $$\displaystyle{\frac{|f(h) - p(h)|}{|h^{n}|}} \leq M \text{ para } h \text{ suficientemente pequeño }$$
        Entonces  se dice que $p(h)$ aproxima a $f(h)$ con orden de aproximación $O(h^{n})$ 
        lo que se escribe como:
      
        $$f(h)= p(h) + O(h^{n})$$
      </p>

      <p><b>De esta manera $p(h) + O(h^{n})$ denota el conjunto de funciones que tienen el crecimiento de $p(x)$ más 
        una parte cuyo crecimiento se limita al de $h^{n}$.</b></p>
    </div>

    
    <div class="nota-box"> <h2 class="number-title"> Observación</h2>
    <p>
      Escribiendo la relación $\displaystyle{\frac{|f(h) - p(h)|}{|h^{n}|}} \leq M$  como $|f(h) - p(h)| \leq M |h^{n}|$,  
      vemos que $O(h^{n})$ ocupa el lugar de la cota de error $M|h^{n}|$. Además, se sigue que $f(h)= p(h) + O(h^{n})$
      es equivalente a $f(h)- p(h) = O(h^{n})$
    </p><br>

   
    </div>
    
    <!--
    <div class="nota-box"> <h2 class="number-title"> Nota</h2>
      <p>
    <p> El siguiente resultado  muestra como podemos aplicar esta definicion a combinaciones simples de funciones:
    </p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Teorema</h2>
      <p>
        Supongamos que $f(h) = p(h) + O(h^{n})$, $g(h)= q(h) + O(h^{m})$ y 
        sea $r=min\{ m,n\}$. Entonces 
        <p>
          <ol>
            <li>$f(h) + g(h) = p(h) + q(h) + O(h^{r})$</li>   <br>         
            <li>$f(h) \cdot  g(h) = p(h) \cdot  q(h) + O(h^{r})$</li><br>
            <li>$\displaystyle{\frac{f(h)}{g(h)}} =\displaystyle{\frac{p(h)}{q(h)}} + O(h^{r}), \text{ siempre que } g(h)\not= 0 \text{ y que } q(h)\not= 0$</li>            
          </ol></p>
      </p>
      <p><b>Demostración:</b> Se deja de ejercicio al lector.</p>
    </div>
-->    
    

    <div class="nota-box"> <h2 class="number-title"> Observación</h2>
      <p>
       Consideremos el caso en el que $p(x)$ es la $n$-ésima aproximación por polinomios de Taylor 
       de $f(x)$; entonces el resto de la fórmula de Taylor se denota  simplemente por $O(h^{n+1})$
       y sustituye a todos los términos omitidos, que son el que contiene la potencia $h^{n+1}$ y los de 
       grado superior. El resto de la fórmula de Taylor converge a cero con la misma rapidez que $h^{n+1}$ converge 
       a cero cuando $h\to 0$, tal y como lo expresa la siguiente relación:

       $$O(h^{n+1})\approx M \cdot h^{n+1} \approx \displaystyle{\frac{f^{n+1}(c)}{(n+1)!}} \cdot h^{n+1}$$
      
       válida para $h$ suficientemente pequeño. En otras palabras, el término $O(h^{n+1})$ sustituye 
       a la cantidad $M \cdot h^{n+1}$, donde $M$ es constante o se "comporta como una constante." 
      </p>
      </div>

      <div class="nota-box"> <h2 class="number-title"> Teorema de Taylor</h2>

        <p>
          Supongamos que $f \in C^{n+1}[a,b]$. Si $x_{0}, x= x_{0} + h \in [a,b]$, entonces 
          $$f(x_{0} + h)= \sum_{k=0}^{n} \displaystyle{\frac{f^{k}(x_{0})}{k!}} h^{k} + O(h^{n+1}) $$
        </p>
      </div>

      <div class="nota-box"> <h2 class="number-title"> Ejemplo</h2>
      <p>
        Consideremos los desarrollos de Taylor: 
      <ul>
        <li>$e^{h}=1 + h + \displaystyle{\frac{h^{2}}{2!}} + \displaystyle{\frac{h^{3}}{3!}} + O(h^{4})$ y </li>
        <li>$Cos(h)= 2 - \displaystyle{\frac{h^{2}}{2!}} + \displaystyle{\frac{h^{4}}{4!}} + O(h^{6})$</li>
      </ul>
      </p>
      <p>Determinar el orden de aproximación de la aproximación para la suma y el producto.</p>
      <p><b>Solución:</b></p>
      </div>

      <h1 class="number-title flexbox"> El orden de aproximación de una sucesión</h1>   


      <div class="nota-box"> <h2 class="number-title"> Observación</h2>
        <p>
          El orden de las aproximaciones númericas se suelen conseguir calculando
          una sucesión de aproximaciones que se acerquen más y más a la respuesta deseada. 
          La definicón de orden de aproximación de una sucesión es analoga a la dada 
          para funciones en la <a href="#Def:aprox_O_funciones"> Definición 1.10</a>  
        </p>
      </div>

      
      <div class="nota-box"> <h2 class="number-title"> Definición</h2>
        <p>
          Supoongamos que $\lim_{n\to \infty} x_{n}=x$ y que $\{r_{n}\}_{n=1}^{\infty}$ 
          es unaa sucesión tal que $\lim_{n\to \infty} r_{n}=0$. Se dice que $\{x_{n}\}_{n=1}^{\infty}$
          converge   a $x$ con orden de aproximación $O(r_{n})$ si existe una constante $K>0$ tal que 
          
          $$\displaystyle{\frac{|x_{n} - x|}{|r^{n}|}} \leq K \text{  para } n \text{ lo suficientemente grande}$$ 
          
          esto se denota por $x_{n}=x+ O(r_{n})$ o bien $x_{n} \to x$ con orden de aproximación $O(r_{n})$
        </p>
      </div>

      <div class="nota-box"> <h2 class="number-title"> Ejemplo</h2>
        <p>
          Sea $x_{n}= \displaystyle{\frac{\cos(n)}{n^2}}$ y $r_{n}= \displaystyle{\frac{1}{n^{2}}}$. Entonces 
          $\lim_{n\to \infty} x_{n}=0$ con orden de aproximación $O(\frac{1}{n^{2}})$ 
        </p>
      </div>


    <div class="nota-box"> <h2 class="number-title"> Observación</h2>

      <p>
        La mayoría de los algoritmos numericos que de veran  a lo largo del curso generan una sucesión  de valores 
        que por lo general  convergen a un número real  $X$.  Los elementos de la sucesión que se llaman aproximaciones 
        se van generando a partir de las aproximaciones anteriores.  
      </p>

      <p>Un criterio para elegir los métodos que se utilizan es que, cambios pequeños en los 
        datos iniciales produzcan otros correspondientes en los resutados finales. Un algoritmo que cumple con esta 
      propiedad se llama <b>estable</b>  en caso contrario se llama <b>inestable</b>
      </p>

      <p>
        Algunos algoritmos que sólo son estables para ciertas elecciones de datos iniciales; a estos se les conoce como 
        <b>condicionalmente estables.</b>
      </p>

      <p>Para estudiar el crecimiento de los errores de redondeo y su relación con la estabilidad de un algoritmo 
        se tiene la siguiente definición:
      </p>

      </div>

      <div class="nota-box"> <h2 class="number-title"> Definición </h2>
        <p>Sea $E_{0} >0$ un error inicial y $E_{n}$ la maginitud de un error despues de $n$ operaciones 
          sucesivas.
        </p>
        <ul>
          <li>Si $E_{n}\approx C_{n} E_{0}$ donde $C$ es una constante independiente de $n$, entonces se dice que 
            el crecimiento del error es <b>lineal</b> </li>
          <li>Si $E_{n}\approx C^{n} E_{0}$ para algun $C>1$, entonces se dice que 
            el crecimiento del error es <b>exponencial</b>  </li>
        </ul>
      </div>



    <div class="nota-box"> <h2 class="number-title"> Definición </h2>
    <p>Particularmente es inevitable el crecimiento lineal de un error y cuando  $C$ y $E_{0}$ 
      son pequeños, por lo general son aceptables los resultados.
    </p>

    <p>Por otro lado siempre se trata de evitar el crecimiento exponencial de error, pues el término 
      $C_{n}$ crece incluso para valore  pequeños de $n$, lo que conduce a empresiones inaceptables.
    </p>

    <p>La siguiente definición proporciona una medida de la rapidez de convergencia de una sucesión:</p>

   </div>


    <div class="nota-box"> <h2 class="number-title"> Definición </h2>
      <p>
        Sea $\{\beta_{n} \}$ una sucesión que converge a cero y $\{\alpha_{n} \}_{n=1}^{\infty}$.
        Una sucesión que converge a a un número $\alpha$. Si existe una constante positiva $K$ tal que 
        para $n$ lo suficientemente grande se satisface:

        $$|\alpha_{n} - \alpha| \leq K |\beta_{n}|$$

        entonces, se dice que la sucesión $\{\alpha_{n} \}_{n=1}^{\infty}$ converge a $\alpha$ 
        con rapidez de convergencia $O(\beta_{n})$. Esta expresión se lee "O mayuscula de $\beta_{n}$"
         y se indica escribiendo:
         
         $$\alpha_{n}= \alpha + O(\beta_{n})$$
        
      </p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Observación </h2>
      <p>Aunque en la definición anterior  se puede utilizar cualquier sucesión 
        $\{\beta_{n} \}_{n=1}^{\infty}$ en casi todas las situaciones se utilizará 
        
        $$\beta_{n}:= \displaystyle{\frac{1}{n^{p}}}$$

        para algún $p>0$. Por lo general se tiene interes en el mayor valor de $p$ 
        tal que:
        
        
        $$\alpha_{n}= \alpha + O\left(\displaystyle{\frac{1}{n^{p}}}\right)$$
      </p>

    </div>

    
    <div class="nota-box"> <h2 class="number-title"> Definición </h2>
      <p>
        Para cada  $n \in \mathbb{N}$, definamos 
        <ul>
          <li>$\alpha_{n}:= \displaystyle{\frac{n+1}{n^{2}}}$</li>
          <li>$\tilde{\alpha_{n}}:= \displaystyle{\frac{n+3}{n^{3}}}$</li>
        </ul>
      </p>
      <p>Determine cual de entre las sucesiones $\{\alpha_{n} \}_{n\geq 1}$ y $\{\tilde{\alpha_{n}} \}_{n\geq 1}$ converge 
      mucho más rapido al cero. 
      </p>
    </div>

    <div class="nota-box"> <h2 class="number-title"> Observación </h2>
      <p>
        Se tiene una definición analoga para la rápidez  de convergencia de funciones  
      </p>
    </div>


    <div class="nota-box"> <h2 class="number-title"> Definición </h2>
      <p>
        Supongamos que $\lim_{h\to a} G(h)=0$ y $\lim_{h\to a} F(h)=0$. Si existe una constante positiva $K$ tal que 
        para $h$ lo suficientemente pequeña, se satisface:

        $$|F(h) - L| \leq K |G(h)|$$

      lo cual se donota por 
      
      $$F(h) = L + O(G(h))$$ 

      Frecuentemente, las funciones que se usan para la comparación tienen la  forma $G(h)= h^{p}$, donde $p >0$.  
      </p>

      <p>Nos interesa  el mayor valor de $p$ para el cual se satisface:

        $$F(h)= L + O(h^{p})$$
      </p>
    </div>

    <h1 class="number-title flexbox"> Ecuaciones no lineales </h1>   

    <div class="nota-box"> <h2 class="number-title"> Introducción </h2>
      <p>
        Para resolver algunos problemas que se presentan en ciencias e ingenieria en ocasiones es necesario conocer 
        los valores que anulan ciertas funciones o ecuaciones y porlo general la teoría que se tiene del álgebra 
        o elcalculo para conocer dihos valores son tediosos o dificiles de aplicar. De aquí la necesidad de desarrolllar 
        métodos o algoritmos que nos permiten en tiempo muy corto solucionar este tipo de problemas. 
      </p>

    </div>


    
    <div class="nota-box"> <h2 class="number-title"> Definición </h2>
      <p>
        una raiz, cero o solucion de la ecuación:
        $$f(x)=0$$
        es cualquier número $\overline{x}$ que cumpla con $f(\overline{x})=0$
      </p>

    </div>

    <div class="nota-box"> <h2 class="number-title"> Introducción </h2>
      <p>
       Entre los métodos que se tienen para resolver la ecuación $f(x)=0$ se encuentran los métodos cerrados y abiertos. 
      </p>

      <p>Los métodos  cerrados necesitan de dos valores iniciales $X_{I}$ y $X_{F}$ que encierran a la raiz $p$
        y tienen la característica  de que siempre encuentran una raíz o en el mejor de los casos una aproximación a $p$.
      </p>

      <p>Los métodos abiertos requieren unicamente de un sólo valor de inicio $p_{0}$  o bien de dos valores iniciales 
        que no necesariamente encierran a la raíz $p$. Estos a veces no encuentran  la raíz pero cuando lo hacen por lo general 
        lo encuentran mucho más rápido que los métodos abiertos. 
      </p>

       </div>

</body>